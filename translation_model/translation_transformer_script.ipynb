{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1K-fpQQjQRDX"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCpkvWdnQRDZ"
      },
      "source": [
        "English to German Translation with `Transformers`\n",
        "========================================================\n",
        "\n",
        "### Datasets\n",
        "    -   Use torchtext library to access\n",
        "        [Multi30k](http://www.statmt.org/wmt16/multimodal-task.html#task1)\n",
        "        dataset to train a German to English translation model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g56X5jgdQRDZ"
      },
      "source": [
        "Data Sourcing and Processing\n",
        "============================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PJf6CpOwQRDa"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import multi30k, Multi30k\n",
        "from typing import Iterable, List\n",
        "\n",
        "\n",
        "# We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
        "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
        "\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'de'\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRnspNzQRDa"
      },
      "source": [
        "Create source and target language tokenizer. Make sure to install the\n",
        "dependencies.\n",
        "\n",
        "``` {.sourceCode .python}\n",
        "pip install -U torchdata\n",
        "pip install -U spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "python -m spacy download de_core_news_sm\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -U -q torchdata\n",
        "%pip install -q portalocker\n",
        "%pip install -U -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNJv2nKZQkkQ",
        "outputId": "9a010808-aa07-4d10-f0ad-3bc9186b56f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9069JeJuQRDa"
      },
      "outputs": [],
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "\n",
        "# helper function to yield list of tokens\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for data_sample in data_iter:\n",
        "        yield token_transform[language](data_sample[language_index[language]])\n",
        "\n",
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    # Training data Iterator\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Create torchtext's Vocab object\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1,\n",
        "                                                    specials=special_symbols,\n",
        "                                                    special_first=True)\n",
        "\n",
        "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
        "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X772pMNiQRDa"
      },
      "source": [
        "Seq2Seq Network using Transformer\n",
        "=================================\n",
        "\n",
        "Transformer is a Seq2Seq model introduced in [\"Attention is all you\n",
        "need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
        "paper for solving machine translation tasks. Below, we will create a\n",
        "Seq2Seq network that uses Transformer. The network consists of three\n",
        "parts. First part is the embedding layer. This layer converts tensor of\n",
        "input indices into corresponding tensor of input embeddings. These\n",
        "embedding are further augmented with positional encodings to provide\n",
        "position information of input tokens to the model. The second part is\n",
        "the actual\n",
        "[Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n",
        "model. Finally, the output of the Transformer model is passed through\n",
        "linear layer that gives unnormalized probabilities for each token in the\n",
        "target language.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jq-G5ZhNQRDb"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJKp3bdQQRDb"
      },
      "source": [
        "During training, we need a subsequent word mask that will prevent the\n",
        "model from looking into the future words when making predictions. We\n",
        "will also need masks to hide source and target padding tokens. Below,\n",
        "let\\'s define a function that will take care of both.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aus0IlZnQRDb"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbRcC5E5QRDb"
      },
      "source": [
        "Let\\'s now define the parameters of our model and instantiate the same.\n",
        "Below, we also define our loss function which is the cross-entropy loss\n",
        "and the optimizer used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWVWYPwvQRDc",
        "outputId": "8ed87f35-2f87-43db-825b-49517414bafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "BATCH_SIZE = 128\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMJXlQ4rQRDd"
      },
      "source": [
        "Collation\n",
        "=========\n",
        "\n",
        "As seen in the `Data Sourcing and Processing` section, our data iterator\n",
        "yields a pair of raw strings. We need to convert these string pairs into\n",
        "the batched tensors that can be processed by our `Seq2Seq` network\n",
        "defined previously. Below we define our collate function that converts a\n",
        "batch of raw strings into batch tensors that can be fed directly into\n",
        "our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8XrmBHJ8QRDd"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                      torch.tensor(token_ids),\n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r0URBxiQRDd"
      },
      "source": [
        "Let\\'s define training and evaluation loop that will be called for each\n",
        "epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Y1SszRNEQRDd"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in train_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44J8g2irQRDd"
      },
      "source": [
        "Now we have all the ingredients to train our model. Let\\'s do it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2Oz0lfBQRDd",
        "outputId": "b4e03ae2-0970-4b8f-d816-113a122b38d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 5.894, Val loss: 4.644, Epoch time = 49.640s\n",
            "Epoch: 2, Train loss: 4.114, Val loss: 3.754, Epoch time = 49.104s\n",
            "Epoch: 3, Train loss: 3.447, Val loss: 3.291, Epoch time = 49.200s\n",
            "Epoch: 4, Train loss: 3.011, Val loss: 2.991, Epoch time = 49.342s\n",
            "Epoch: 5, Train loss: 2.684, Val loss: 2.807, Epoch time = 49.316s\n",
            "Epoch: 6, Train loss: 2.423, Val loss: 2.639, Epoch time = 50.278s\n",
            "Epoch: 7, Train loss: 2.216, Val loss: 2.473, Epoch time = 49.781s\n",
            "Epoch: 8, Train loss: 2.039, Val loss: 2.347, Epoch time = 49.533s\n",
            "Epoch: 9, Train loss: 1.885, Val loss: 2.263, Epoch time = 49.370s\n",
            "Epoch: 10, Train loss: 1.750, Val loss: 2.219, Epoch time = 49.431s\n",
            "Epoch: 11, Train loss: 1.628, Val loss: 2.167, Epoch time = 50.165s\n",
            "Epoch: 12, Train loss: 1.524, Val loss: 2.180, Epoch time = 50.078s\n",
            "Epoch: 13, Train loss: 1.432, Val loss: 2.121, Epoch time = 49.403s\n",
            "Epoch: 14, Train loss: 1.343, Val loss: 2.114, Epoch time = 49.544s\n",
            "Epoch: 15, Train loss: 1.264, Val loss: 2.057, Epoch time = 51.992s\n",
            "Epoch: 16, Train loss: 1.191, Val loss: 2.011, Epoch time = 57.276s\n",
            "Epoch: 17, Train loss: 1.122, Val loss: 1.999, Epoch time = 52.406s\n",
            "Epoch: 18, Train loss: 1.056, Val loss: 1.999, Epoch time = 51.006s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "NUM_EPOCHS = 18\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    training_loss.append(train_loss)\n",
        "    validation_loss.append(val_loss)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Plot train & val loss curve"
      ],
      "metadata": {
        "id": "_gF-9NhvsRxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(training_loss, label='training loss')\n",
        "plt.plot(validation_loss, label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k7DmmaMSsVFX",
        "outputId": "6cea18d3-5954-4ba2-9d1b-60bd25609a72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print number of parameters\n",
        "def count_parameters(model):\n",
        "    \"\"\"Calculates the total number of parameters in a PyTorch model.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The PyTorch model to analyze.\n",
        "\n",
        "    Returns:\n",
        "        int: The total number of parameters in the model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def print_number_of_parameters(_model):\n",
        "    \"\"\"_summary_\n",
        "    Prints the number of parameters in the model.\n",
        "    \"\"\"\n",
        "    _model.eval()\n",
        "\n",
        "    _count = count_parameters(_model)\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"Number of parameters in the model: {_count}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print_number_of_parameters(transformer)"
      ],
      "metadata": {
        "id": "FawecylgSR4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac7a03a-b72e-493f-974a-d1cbfa9d0e7f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Number of parameters in the model: 37468272\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CYK_ZF6QRDd",
        "outputId": "a61095c7-9332-4468-e089-3f2e5bdbabd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Eine Gruppe von Menschen steht vor einem Iglu . \n"
          ]
        }
      ],
      "source": [
        "# Eine Gruppe von Menschen steht vor einem Iglu .\n",
        "print(translate(transformer, \"A group of people standing in front of an igloo \"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(transformer.state_dict(), \"model_english_to_german.pt\")"
      ],
      "metadata": {
        "id": "YV6Jo0m4a8S0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuE8jrKGbZFf",
        "outputId": "39cf92cc-dcaf-4417-ef05-ad9bdd29858f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 153M\n",
            "-rw-r--r-- 1 root root 153M Mar 21 20:30 model_english_to_german.pt\n",
            "drwxr-xr-x 1 root root 4.0K Mar 20 14:00 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zV7ze9hQRDd"
      },
      "source": [
        "References\n",
        "==========\n",
        "\n",
        "1.  Attention is all you need paper.\n",
        "    <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>\n",
        "2.  The annotated transformer.\n",
        "    <https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}